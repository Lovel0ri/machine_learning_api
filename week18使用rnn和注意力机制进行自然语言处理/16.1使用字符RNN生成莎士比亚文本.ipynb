{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPXmXKzGIcYBvE6MSfDjc7v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## 创建训练数据集"],"metadata":{"id":"UOGVF-IQyi4r"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"a1DVB83PyYCb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684307384373,"user_tz":-480,"elapsed":6646,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"c5d2c516-c87a-4b4c-fb83-4694e7b615b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://homl.info/shakespeare\n","1115394/1115394 [==============================] - 0s 0us/step\n"]}],"source":["import keras\n","shakespeare_url = \"https://homl.info/shakespeare\"\n","filepath = keras.utils.get_file(\"shakespeare.txt\",shakespeare_url)\n","with open(filepath) as f:\n","  shakespeare_text = f.read()"]},{"cell_type":"code","source":["!pip install keras-preprocessing --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ax8ZG7Jgzpse","executionInfo":{"status":"ok","timestamp":1684307389809,"user_tz":-480,"elapsed":5440,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"311a38e4-9e31-497c-e0ca-b3f51b23445e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras-preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.22.4)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n","Installing collected packages: keras-preprocessing\n","Successfully installed keras-preprocessing-1.1.2\n"]}]},{"cell_type":"code","source":["\n","from keras_preprocessing import text\n","\n","tokenizer = text.Tokenizer(char_level=True)\n","tokenizer.fit_on_texts([shakespeare_text])\n","sequences = tokenizer.texts_to_sequences([\"First\"])\n","\n","\n"],"metadata":{"id":"g6xNAjG0zDY_","executionInfo":{"status":"ok","timestamp":1684307389810,"user_tz":-480,"elapsed":9,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["sequences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSY5eKGSz3uc","executionInfo":{"status":"ok","timestamp":1684307389810,"user_tz":-480,"elapsed":8,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"904a6770-771a-47a4-cf47-255660da341f"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[20, 6, 9, 8, 3]]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKQt2sE40eMB","executionInfo":{"status":"ok","timestamp":1684307389810,"user_tz":-480,"elapsed":6,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"4d49f7cb-e463-42eb-ef91-a19865724037"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['f i r s t']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["max_id = len(tokenizer.word_index)\n","max_id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjnTAQDU0zwB","executionInfo":{"status":"ok","timestamp":1684307389810,"user_tz":-480,"elapsed":5,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"4dc0fd8e-ec51-470b-d691-a1da093d4266"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["dataset_size = tokenizer.document_count\n","dataset_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RS_2CiHd3Lsm","executionInfo":{"status":"ok","timestamp":1684307389811,"user_tz":-480,"elapsed":5,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"be373b83-d0cd-4891-aa4e-7d60597c42b7"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import numpy as np\n","[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) -1\n","encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQ7rc4KV3Ujp","executionInfo":{"status":"ok","timestamp":1684307389811,"user_tz":-480,"elapsed":4,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"683193a7-afc1-4d09-d7ed-0c95302a2fc9"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([19,  5,  8, ..., 20, 26, 10])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## 如何拆分顺序数据集"],"metadata":{"id":"8HOG4SOBKEiZ"}},{"cell_type":"code","source":["train_size = dataset_size * 90 // 100"],"metadata":{"id":"tr6e1gCr3je0","executionInfo":{"status":"ok","timestamp":1684307390948,"user_tz":-480,"elapsed":6,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"],"metadata":{"id":"x370bdA0LwBw","executionInfo":{"status":"ok","timestamp":1684307390948,"user_tz":-480,"elapsed":6,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## 将顺序数据集切成多个窗口"],"metadata":{"id":"hCStJoG8N1iw"}},{"cell_type":"code","source":["n_steps = 100\n","window_length = n_steps + 1\n","dataset  = dataset.window(window_length,shift=1,drop_remainder=True)"],"metadata":{"id":"N1rbV8h_L-yw","executionInfo":{"status":"ok","timestamp":1684307390948,"user_tz":-480,"elapsed":5,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## 展平数据集"],"metadata":{"id":"74jF_t2oC8rM"}},{"cell_type":"code","source":["dataset = dataset.flat_map(lambda window: window.batch(window_length))"],"metadata":{"id":"0zKHHsTAOCbX","executionInfo":{"status":"ok","timestamp":1684307390949,"user_tz":-480,"elapsed":6,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","dataset = dataset.shuffle(10000).batch(batch_size)\n","dataset = dataset.map(lambda windows:(windows[:,:-1],windows[:,1:]))"],"metadata":{"id":"loeVyJCVEiw1","executionInfo":{"status":"ok","timestamp":1684307390949,"user_tz":-480,"elapsed":5,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.map(\n","    lambda X_batch,Y_batch:(tf.one_hot(X_batch,depth=max_id),Y_batch)\n",")\n","dataset = dataset.prefetch(1)"],"metadata":{"id":"1hzsBWA5V4IJ","executionInfo":{"status":"ok","timestamp":1684307390949,"user_tz":-480,"elapsed":5,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## 创建和训练Char-RNN模型"],"metadata":{"id":"wLYg3VoJYJCF"}},{"cell_type":"code","source":["model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2, recurrent_dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True, dropout=0.2,\n","                     recurrent_dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","])\n","\n"],"metadata":{"id":"ooujdyz5YCex","executionInfo":{"status":"ok","timestamp":1684307391649,"user_tz":-480,"elapsed":705,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# model.compile(loss=\"sprase_categorical_corssentropy\",optimizer=\"adam\")\n","# history = model.fit(dataset,epochs=10)"],"metadata":{"id":"GYZyTZI3eGzV","executionInfo":{"status":"ok","timestamp":1684307391649,"user_tz":-480,"elapsed":7,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# tf.config.run_functions_eagerly(True) # 启用 run_eagerly 模式\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", run_eagerly=True)\n","history = model.fit(dataset, epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"id":"5ZoDpPKQecd9","executionInfo":{"status":"error","timestamp":1684307517026,"user_tz":-480,"elapsed":573,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"37b41797-15cb-4b24-fbe3-241dfcace318"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-16fd5ba4aa5d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1695\u001b[0m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1698\u001b[0m                         \u001b[0;34m\"Unexpected result of `train_function` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m                         \u001b[0;34m\"(Empty logs). Please use \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."]}]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nV6rf8ikhNJl","executionInfo":{"status":"ok","timestamp":1684307453707,"user_tz":-480,"elapsed":691,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"c290351e-e5d1-458a-e643-34eb216511ed"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 39), dtype=tf.float32, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","\n","# 加载和处理数据\n","def load_data():\n","    import keras\n","    shakespeare_url = \"https://homl.info/shakespeare\"\n","    filepath = keras.utils.get_file(\"shakespeare.txt\",shakespeare_url)\n","    with open(filepath) as f:\n","        text = f.read()\n","        # 转为小写\n","        text = text.lower()\n","        # 去除标点符号\n","        text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n","    # 构建字符表\n","    chars = sorted(set(text))\n","    char_to_index = {c: i for i, c in enumerate(chars)}\n","    index_to_char = {i: c for i, c in enumerate(chars)}\n","    # 将文本转换为数字\n","    encoded_text = [char_to_index[c] for c in text]\n","    return encoded_text, char_to_index, index_to_char\n","\n","\n","# 生成批次数据\n","def get_batch(encoded_text, batch_size, seq_length):\n","    # 计算每个批次中字符的数量\n","    n = batch_size * seq_length\n","    # 将编码后的文本转为矩阵\n","    batches = np.array(encoded_text[:len(encoded_text) // n * n], dtype=np.int32)\n","    # 设定输入和目标矩阵\n","    x = batches.reshape([batch_size, -1])\n","    y = np.roll(batches, -1).reshape([batch_size, -1])\n","    # 分割每个批次的矩阵\n","    for i in range(0, x.shape[1], seq_length):\n","        yield x[:, i: i + seq_length], y[:, i: i + seq_length]\n","\n","\n","# 定义Char-RNN模型\n","class CharRNN(tf.keras.Model):\n","    def __init__(self, num_chars, embedding_size, hidden_size):\n","        super().__init__()\n","        self.embedding = tf.keras.layers.Embedding(num_chars, embedding_size)\n","        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n","        self.dense = tf.keras.layers.Dense(num_chars)\n","\n","    def call(self, inputs, states=None, training=False):\n","        # 输入经过嵌入层\n","        x = self.embedding(inputs)\n","        # LSTM单元处理输入和状态\n","        output, state_h, state_c = self.lstm(x, initial_state=states)\n","        # 全连接层输出各字符的概率分布\n","        logits = self.dense(output)\n","        return logits, state_h, state_c\n","\n","\n","# 训练模型\n","def train():\n","    # 加载和处理数据\n","    encoded_text, char_to_index, index_to_char = load_data()\n","    num_chars = len(char_to_index)\n","    # 定义训练超参数\n","    batch_size = 32\n","    seq_length = 100\n","    learning_rate = 0.01\n","    num_epochs = 5\n","    # 创建模型\n","    model = CharRNN(num_chars, 128, 256)\n","    # 定义损失函数和优化器\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate)\n","    # 定义检查点对象\n","    checkpoint_directory = './training_checkpoints'\n","    checkpoint_prefix = os.path.join(checkpoint_directory, 'ckpt_{epoch}')\n","    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n","\n","    # 迭代训练模型\n","    for epoch in range(num_epochs):\n","        states = None\n","        for batch_x, batch_y in get_batch(encoded_text, batch_size, seq_length):\n","            with tf.GradientTape() as tape:\n","                # 计算模型输出和损失\n","                logits, state_h, state_c = model(batch_x, states=states, training=True)\n","                loss = loss_fn(batch_y, logits)\n","            # 计算梯度并更新模型参数\n","            grads = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","            states = (state_h, state_c)\n","        # 保存检查点\n","        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n","        # 输出每个轮次的损失值\n","        print('Epoch {}, Loss {:.4f}'.format(epoch + 1, loss))\n","\n","    return model, index_to_char\n","\n","\n","def predict(model, index_to_char, char_to_index, seed_text, length):\n","    \"\"\"\n","    使用训练好的 Char-RNN 模型生成新文本\n","\n","    参数:\n","    - model: 训练好的 Char-RNN 模型\n","    - index_to_char: 将数字映射回字符的字典\n","    - char_to_index: 将字符映射到数字的字典\n","    - seed_text: 用于生成新文本的种子文本\n","    - length: 生成文本的长度\n","    \"\"\"\n","\n","    # 将种子文本转换为数字\n","    x = [char_to_index[c] for c in seed_text]\n","    x = tf.expand_dims(x, 0)\n","\n","    # 初始化状态\n","    states = None\n","\n","    # 生成新文本\n","    for i in range(length):\n","        # 在模型上执行一步推理，并获取预测结果和新状态\n","        logits, state_h, state_c = model(x, states=states, training=False)\n","\n","        # 将 logits 转换为概率分布，并从中采样\n","        logits = tf.squeeze(logits, 0)\n","        prob = tf.nn.softmax(logits / 0.5).numpy()\n","        index = np.random.choice(len(index_to_char), p=prob)\n","\n","        # 添加新字符\n","        x = tf.expand_dims([index], 0)\n","\n","        # 更新状态\n","        states = (state_h, state_c)\n","\n","        # 输出新字符\n","        print(index_to_char[index], end='')\n","\n","\n","# 载入数据\n","encoded_text, char_to_index, index_to_char = load_data()\n","\n","# 训练模型\n","model, _ = train()\n","\n","predict(model, index_to_char, char_to_index, \"shall i compare thee to a summer's day?\\n\", 500)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"pXI6XIjojCDK","executionInfo":{"status":"error","timestamp":1684308703588,"user_tz":-480,"elapsed":57583,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"8cf83457-56bf-4464-a782-961d4bd6c84e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss 1.5997\n","Epoch 2, Loss 1.4801\n","Epoch 3, Loss 1.4317\n","Epoch 4, Loss 1.4129\n","Epoch 5, Loss 1.4001\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-9909d40d1bd6>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_to_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shall i compare thee to a summer's day?\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-9909d40d1bd6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, index_to_char, char_to_index, seed_text, length)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# 将种子文本转换为数字\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-9909d40d1bd6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# 将种子文本转换为数字\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"'\""]}]},{"cell_type":"code","source":["def preprocess(texts):\n","  "],"metadata":{"id":"dqdRTZ4TnuGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2, recurrent_dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True, dropout=0.2,\n","                     recurrent_dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","])\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", run_eagerly=True)\n","history = model.fit(dataset, epochs=10)"],"metadata":{"id":"0hqjDt6FlnTG"},"execution_count":null,"outputs":[]}]}