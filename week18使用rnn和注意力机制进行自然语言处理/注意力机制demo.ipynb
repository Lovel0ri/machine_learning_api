{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxyKUOk0irfH6CwQEFTQjv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"id":"XALUd1RmD0Jc","executionInfo":{"status":"ok","timestamp":1684920437037,"user_tz":-480,"elapsed":470,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, LSTM, Flatten, Lambda, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError\n","import numpy as np\n","from numpy.random import randint\n"]},{"cell_type":"markdown","source":["然后，我们定义一个名为 “AttentionLayer” 的新层，该层将实现注重注意力机制。"],"metadata":{"id":"ez5eMx1CD4Yr"}},{"cell_type":"code","source":["class AttentionLayer(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(AttentionLayer, self).__init__()\n","\n","    def build(self, input_shape):\n","        self.w1 = self.add_weight(\n","            shape=(input_shape[-1], 1), initializer=\"random_normal\", trainable=True\n","        )\n","        self.b1 = self.add_weight(\n","            shape=(input_shape[1], 1), initializer=\"zeros\", trainable=True\n","        )\n","\n","    def call(self, inputs):\n","        e = tf.matmul(inputs, self.w1) + self.b1\n","        alpha = tf.nn.softmax(e, axis=1)\n","        output = inputs * alpha\n","        return tf.reduce_sum(output, axis=1)"],"metadata":{"id":"vDLlnxaFD6Tg","executionInfo":{"status":"ok","timestamp":1684920437038,"user_tz":-480,"elapsed":2,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.losses import CategoricalCrossentropy\n","\n","inputs = Input(shape=(10, 8))\n","\n","lstm_out = LSTM(50, return_sequences=True, dropout=0.1)(inputs)\n","attention_out = AttentionLayer()(lstm_out)\n","\n","concatenated_out = Concatenate()([attention_out, Flatten()(lstm_out[:, -1, :])])\n","dense_out = Dense(10, activation=\"relu\")(concatenated_out)\n","outputs = Dense(8, activation=\"softmax\")(dense_out)\n","\n","model = Model(inputs, outputs)\n","model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n","\n","X = np.random.uniform(size=(1000,8))\n","y = X[:,:]\n","\n","model.fit(X, y, epochs=10, batch_size=32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":628},"id":"RQU7tqTzFBBu","executionInfo":{"status":"error","timestamp":1684920468863,"user_tz":-480,"elapsed":1096,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"8d5d4fd8-d604-4b63-99d7-09da5c062d1d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-32883914fdf8>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_6\" is incompatible with the layer: expected shape=(None, 10, 8), found shape=(None, 8)\n"]}]},{"cell_type":"markdown","source":["接下来，我们定义模型的输入层，输入大小为(10,8)。然后，我们定义一个基于LSTM的双向RNN，以捕获输入序列中的时间依赖关系。随后，我们添加一层AttentionLayer（上面定义的自定义层），它将加强我们模型的注意力机制。最后，我们将添加一些全连接层，以将模型的输出映射到所需维数。"],"metadata":{"id":"TkyOp6JcEBic"}},{"cell_type":"code","source":["inputs = Input(shape=(10, 8))\n","\n","lstm_out = LSTM(50, return_sequences=True, dropout=0.1)(inputs)\n","attention_out = AttentionLayer()(lstm_out)\n","\n","concatenated_out = Concatenate()([attention_out, Flatten()(lstm_out[:, -1, :])])\n","dense_out = Dense(10, activation=\"relu\")(concatenated_out)\n","outputs = Dense(8, activation=\"softmax\")(dense_out)\n","\n","model = Model(inputs, outputs)\n"],"metadata":{"id":"03XfppWAEB8-","executionInfo":{"status":"ok","timestamp":1684920322022,"user_tz":-480,"elapsed":1136,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["这个模型的训练是基于一些模拟数据的，我们先生成了一组大小为 (1000,10,8)的随机数据用来训练我们的模型。接下来，反转了这些数据中的输入序列，然后用反转后的序列作为模型期望的输出。最后，我们使用Adam优化器和均方误差损失函数编译该模型，并通过模型的 fit() 方法对其进行训练。"],"metadata":{"id":"mllZjLrOETA9"}},{"cell_type":"code","source":["model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n","\n","X = np.random.uniform(size=(1000, 10, 8))\n","y = np.reshape(X[:, ::-1, :], (-1, 8))\n","X = np.repeat(X, repeats=10, axis=0)\n","\n","model.fit(X, y, epochs=10, batch_size=32)\n"],"metadata":{"id":"e_WaIbBcEP2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.layers import Input, Dense, LSTM, Flatten, Lambda, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","\n","class AttentionLayer(tf.keras.layers.Layer):\n","\n","    def __init__(self):\n","        super(AttentionLayer, self).__init__()\n","\n","    def build(self, input_shape):\n","        self.w1 = self.add_weight(\n","            shape=(input_shape[-1], 1), initializer=\"random_normal\", trainable=True\n","        )\n","        self.b1 = self.add_weight(\n","            shape=(input_shape[1], 1), initializer=\"zeros\", trainable=True\n","        )\n","\n","    def call(self, inputs):\n","        e = tf.matmul(inputs, self.w1) + self.b1\n","        alpha = tf.nn.softmax(e, axis=1)\n","        output = inputs * alpha\n","        return tf.reduce_sum(output, axis=1)\n","\n","inputs = Input(shape=(10, 8))\n","\n","lstm_out = LSTM(50, return_sequences=True, dropout=0.1)(inputs)\n","attention_out = AttentionLayer()(lstm_out)\n","\n","concatenated_out = Concatenate()([attention_out, Flatten()(lstm_out[:, -1, :])])\n","dense_out = Dense(10, activation=\"relu\")(concatenated_out)\n","outputs = Dense(8, activation=\"softmax\")(dense_out)\n","\n","model = Model(inputs, outputs)\n","model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n","\n","X = np.random.uniform(size=(1000, 10, 8))\n","y = np.reshape(X[:, ::-1, :], (-1, 8))\n","X = np.repeat(X, repeats=10, axis=0)\n","\n","model.fit(X, y, epochs=10, batch_size=32)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8TC8JE5vEdGl","executionInfo":{"status":"ok","timestamp":1684920686390,"user_tz":-480,"elapsed":48890,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"86d394fb-1766-411c-b59a-a05eeecc4239"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","313/313 [==============================] - 10s 11ms/step - loss: 81.6165\n","Epoch 2/10\n","313/313 [==============================] - 5s 15ms/step - loss: 62.1427\n","Epoch 3/10\n","313/313 [==============================] - 3s 10ms/step - loss: 133.1007\n","Epoch 4/10\n","313/313 [==============================] - 3s 9ms/step - loss: 173.6065\n","Epoch 5/10\n","313/313 [==============================] - 3s 11ms/step - loss: 205.0278\n","Epoch 6/10\n","313/313 [==============================] - 4s 11ms/step - loss: 262.2203\n","Epoch 7/10\n","313/313 [==============================] - 3s 9ms/step - loss: 317.6843\n","Epoch 8/10\n","313/313 [==============================] - 3s 9ms/step - loss: 330.9338\n","Epoch 9/10\n","313/313 [==============================] - 3s 11ms/step - loss: 400.2613\n","Epoch 10/10\n","313/313 [==============================] - 4s 13ms/step - loss: 405.4494\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fca4a18f670>"]},"metadata":{},"execution_count":3}]}]}