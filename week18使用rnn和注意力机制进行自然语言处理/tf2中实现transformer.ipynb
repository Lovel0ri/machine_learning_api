{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMOFGD0evXsK1mWwqWKg6gG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","BUFFER_SIZE = 20000\n","BATCH_SIZE = 64\n","NUM_EPOCHS = 20\n","HEAD_NUM = 8\n","D_MODEL = 512\n","DFF = 2048\n","NUM_LAYERS = 6\n","DROPOUT_RATE = 0.1\n","VOCAB_SIZE_EN = 2**13\n","VOCAB_SIZE_PT = 2**13\n","MAX_LENGTH=40\n","tf.random.set_seed(1234)\n"],"metadata":{"id":"G89AtJxuw3kj","executionInfo":{"status":"ok","timestamp":1684932540603,"user_tz":-480,"elapsed":12865,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n","                              as_supervised=True)\n","\n","train_dataset = examples['train']\n","val_dataset = examples['validation']\n"],"metadata":{"id":"MY5I6wJxxkQZ","executionInfo":{"status":"ok","timestamp":1684932540604,"user_tz":-480,"elapsed":10,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def encode(lang1, lang2):\n","  lang1 = tf.strings.unicode_split(lang1, 'UTF-8')\n","  lang2 = tf.strings.unicode_split(lang2, 'UTF-8')\n","\n","  lang1 = vocab_pt.lookup(lang1)\n","  lang2 = vocab_en.lookup(lang2)\n","\n","  return lang1, lang2\n","\n","def tf_encode(pt, en):\n","    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n","\n","vocab_pt = tf.keras.layers.experimental.preprocessing.StringLookup(\n","    max_tokens=VOCAB_SIZE_PT, mask_token=None)\n","vocab_en = tf.keras.layers.experimental.preprocessing.StringLookup(\n","    max_tokens=VOCAB_SIZE_EN, mask_token=None)\n","\n","train_dataset = train_dataset.map(tf_encode)\n","train_dataset = train_dataset.filter(lambda x, y: tf.logical_and(tf.size(x) <= MAX_LENGTH, tf.size(y) <= MAX_LENGTH))\n","train_dataset = train_dataset.cache()\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","val_dataset = val_dataset.map(tf_encode)\n","val_dataset = val_dataset.filter(lambda x, y: tf.logical_and(tf.size(x) <= MAX_LENGTH, tf.size(y) <= MAX_LENGTH))\n","\n","\n","val_dataset = val_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n"],"metadata":{"id":"6E9OGiwFxvF1","executionInfo":{"status":"ok","timestamp":1684932579674,"user_tz":-480,"elapsed":1618,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def positional_encoding(position, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))\n","\n","    angle_rads = np.arange(position)[:, np.newaxis] * angle_rates[np.newaxis, :]\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) # 2i\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) # 2i+1\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n"],"metadata":{"id":"ym0f9naHzR1o","executionInfo":{"status":"ok","timestamp":1684932582639,"user_tz":-480,"elapsed":381,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def create_model(vocab_size, head_num, d_model, dff, num_layers, dropout_rate):\n","    inputs = tf.keras.layers.Input(shape=(None,))\n","    padding_mask = tf.keras.layers.Lambda(lambda x: tf.cast(tf.math.equal(x, 0), dtype=tf.float32))(inputs)\n","\n","    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","\n","    positional_encoding = positional_encoding(embeddings.shape[1], d_model)\n","    embeddings += positional_encoding\n","\n","    outputs = embeddings\n","    for i in range(num_layers):\n","        outputs = encoder_layer(outputs, head_num, d_model, dff, dropout_rate, padding_mask)\n","\n","    outputs = tf.keras.layers.Dense(vocab_size)(outputs)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","def encoder_layer(inputs, head_num, d_model, dff, dropout_rate, padding_mask):\n","    attention = MultiHeadAttention(head_num, d_model)([inputs, inputs, inputs, padding_mask])\n","    attention = tf.keras.layers.Dropout(dropout_rate)(attention)\n","    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n","\n","    outputs = tf.keras.layers.Dense(dff, activation='relu')(attention)\n","    outputs = tf.keras.layers.Dense(d_model)(outputs)\n","    outputs = tf.keras.layers.Dropout(dropout_rate)(outputs)\n","    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n","\n","    return outputs\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, head_num, d_model):\n","        super(MultiHeadAttention, self).__init__()\n","        self.head_num = head_num\n","        self.d_model = d_model\n","\n","        assert d_model % head_num == 0\n","\n","        self.depth = d_model // head_num\n","\n","        self.dense_q = tf.keras.layers.Dense(d_model)\n","        self.dense_k = tf.keras.layers.Dense(d_model)\n","        self.dense_v = tf.keras.layers.Dense(d_model)\n","\n","        self.dense_output = tf.keras.layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.head_num, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        q, k, v, padding_mask = inputs\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.dense_q(q)\n","        k = self.dense_k(k)\n","        v = self.dense_v(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, padding_mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","\n","        outputs = self.dense_output(concat_attention)\n","\n","        return outputs\n"],"metadata":{"id":"yvU-Je5nxw3n","executionInfo":{"status":"ok","timestamp":1684932583823,"user_tz":-480,"elapsed":3,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["在模型实例化后，我们可以使用Adam优化器和SparseCategoricalCrossentropy损失函数对模型进行编译。\n"],"metadata":{"id":"Jq6ouB4bx3np"}},{"cell_type":"code","source":["model = create_model(VOCAB_SIZE_PT, HEAD_NUM, D_MODEL, DFF, NUM_LAYERS, DROPOUT_RATE)\n","optimizer = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","model.compile(optimizer=optimizer, loss=loss_function)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"id":"5Gguyxz5x1C-","executionInfo":{"status":"error","timestamp":1684932588222,"user_tz":-480,"elapsed":646,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"c6138eb1-c107-40fc-a7dc-a3ea0e262c24"},"execution_count":6,"outputs":[{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-7bca25ecce33>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE_PT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHEAD_NUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDFF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.98\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-e437d3fb3c0d>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(vocab_size, head_num, d_model, dff, num_layers, dropout_rate)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpositional_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'positional_encoding' referenced before assignment"]}]},{"cell_type":"code","source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","val_loss = tf.keras.metrics.Mean(name='val_loss')\n","val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n","\n","for epoch in range(NUM_EPOCHS):\n","  print('Epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n","  start = time.time()\n","\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  val_loss.reset_states()\n","  val_accuracy.reset_states()\n","\n","  for (batch, (inputs, targets)) in enumerate(train_dataset):\n","      with tf.GradientTape() as tape:\n","          predictions = model(inputs, training=True)\n","          loss = loss_function(targets, predictions)\n","\n","      gradients = tape.gradient(loss, model.trainable_variables)\n","      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","      train_loss(loss)\n","      train_accuracy(targets, predictions)\n","\n","      if batch % 50 == 0:\n","          print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch+1, batch, train_loss.result(), train_accuracy.result()))\n","\n","  for (batch, (inputs, targets)) in enumerate(val_dataset):\n","      predictions = model(inputs, training=False)\n","      loss = loss_function(targets, predictions)\n","\n","      val_loss(loss)\n","      val_accuracy(targets, predictions)\n","\n","      if batch % 50 == 0:\n","        print('Epoch {} Validation Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch+1, batch, val_loss.result(), val_accuracy.result()))\n","\n","  print('Epoch {} Loss {:.4f} Accuracy {:.4f} Validation Loss {:.4f} Validation Accuracy {:.4f}'.format(epoch+1, train_loss.result(), train_accuracy.result(), val_loss.result(), val_accuracy.result()))\n","  print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":804},"id":"VB8VlfJsx3Jw","executionInfo":{"status":"error","timestamp":1684932593744,"user_tz":-480,"elapsed":4,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"46af8521-8ea8-4599-bd47-502ac84eac1a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"error","ename":"UnknownError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-10d7ba81b695>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mval_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    781\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3014\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3015\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3016\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3017\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} AttributeError: 'StringLookup' object has no attribute 'lookup'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 265, in __call__\n    return func(device, token, args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 143, in __call__\n    outputs = self._call(device, args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 150, in _call\n    ret = self._func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-3-7d042bbb49da>\", line 5, in encode\n    lang1 = vocab_pt.lookup(lang1)\n\nAttributeError: 'StringLookup' object has no attribute 'lookup'\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]"]}]},{"cell_type":"code","source":["def evaluate(inp_sentence):\n","    start_token = [vocab_pt.vocab_size]\n","    end_token = [vocab_pt.vocab_size + 1]\n","\n","    inp_sentence = start_token + vocab_pt.encode(inp_sentence) + end_token\n","    encoder_input = tf.expand_dims(inp_sentence, 0)\n","\n","    decoder_input = [vocab_en.vocab_size]\n","    output = tf.expand_dims(decoder_input, 0)\n","\n","    for i in range(MAX_LENGTH):\n","        predictions = model(inputs=[encoder_input, output], training=False)\n","\n","        predictions = predictions[:, -1:, :]\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        if predicted_id == vocab_en.vocab_size+1:\n","            return tf.squeeze(output, axis=0)\n","\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0)\n","\n","def translate(sentence):\n","    result = evaluate(sentence).numpy()\n","\n","    predicted_sentence = vocab_en.decode([i for i in result if i < VOCAB_SIZE_EN])\n","\n","    print('Input: {}'.format(sentence))\n","    print('Output: {}'.format(predicted_sentence))\n"],"metadata":{"id":"WMer0kT3yIRC"},"execution_count":null,"outputs":[]}]}