{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMLId/kYvypuQgvBaJmd29v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import requests\n","import zipfile\n","\n","url = 'https://www.manythings.org/anki/fra-eng.zip'\n","r = requests.get(url)\n","\n","with open('fra-eng.zip', 'wb') as f:\n","    f.write(r.content)\n","\n","# with zipfile.ZipFile('fra-eng.zip', 'r') as zip_ref:\n","#     zip_ref.extractall('.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"id":"pv3ImO1q7zZW","executionInfo":{"status":"error","timestamp":1684917954933,"user_tz":-480,"elapsed":5,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"a8b694e0-b88a-4893-c6db-25eff99f4720"},"execution_count":2,"outputs":[{"output_type":"error","ename":"BadZipFile","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-cf5985526b4b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     f.write(r.content)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fra-eng.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"]}]},{"cell_type":"code","source":["!pip install -q torchtext spacy==3\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download fr_core_news_sm\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9QBjyw1ATjx","executionInfo":{"status":"ok","timestamp":1684919562825,"user_tz":-480,"elapsed":453963,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"64c59e4f-502f-4819-d330-5c503fa6b82c"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for spacy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for spacy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for spacy\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not build wheels for spacy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m2023-05-24 09:12:17.605832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","2023-05-24 09:12:31.870668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fr-core-news-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.5.0/fr_core_news_sm-3.5.0-py3-none-any.whl (16.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.5.0) (3.5.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.9)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.1.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.10.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.1.2)\n","Installing collected packages: fr-core-news-sm\n","Successfully installed fr-core-news-sm-3.5.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('fr_core_news_sm')\n"]}]},{"cell_type":"code","source":["!zip /content/fra-eng.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JdMxp2D78zb","executionInfo":{"status":"ok","timestamp":1684917983166,"user_tz":-480,"elapsed":419,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"a2cfd585-4d47-47ae-efd8-1898ff0427b6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\tzip warning: missing end signature--probably not a zip file (did you\n","\tzip warning: remember to use binary mode when you transferred it?)\n","\tzip warning: (if you are trying to read a damaged archive try -F)\n","\n","zip error: Zip file structure invalid (/content/fra-eng.zip)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","import re\n","\n","# 读入数据集\n","data_path = \"./fra.txt\"\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split('\\n')\n","\n","# 清洗数据\n","def preprocess_sentence(sentence):\n","    \"\"\"\n","    Preprocesses a sentence by converting to lowercase, removing punctuation and non-alphabetic characters,\n","    and standardizing whitespace.\n","    \"\"\"\n","    sentence = sentence.lower().strip()\n","    sentence = re.sub(r\"[^a-zA-ZÀ-ÿ\\s']\", \"\", sentence)\n","    sentence = re.sub(r\"\\s+\", \" \", sentence)\n","    sentence = sentence.strip()\n","    return sentence\n","\n","# 划分训练和测试集\n","def split_train_test(pair_list, test_size=0.1):\n","    \"\"\"\n","    Splits a list of sentence pairs into train and test sets.\n","    \"\"\"\n","    data = pd.DataFrame(pair_list, columns=[\"eng\", \"fra\"])\n","    train, test = train_test_split(data, test_size=test_size)\n","\n","    eng_train = list(train[\"eng\"].values)\n","    fra_train = list(train[\"fra\"].values)\n","    eng_test = list(test[\"eng\"].values)\n","    fra_test = list(test[\"fra\"].values)\n","\n","    return eng_train, fra_train, eng_test, fra_test\n","\n","# 建立英语和法语的词汇表\n","def build_tokenizer(sentence_list):\n","    \"\"\"\n","    Builds a tokenizer and returns the tokenizer and the size of the vocabulary.\n","    \"\"\"\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=True)\n","    tokenizer.fit_on_texts(sentence_list)\n","    vocab_size = len(tokenizer.word_index) + 1\n","    return tokenizer, vocab_size\n","\n","# 将文本转换为数字\n","def texts_to_sequences(texts, tokenizer):\n","    \"\"\"\n","    Converts a list of texts to a padded matrix of sequences.\n","    \"\"\"\n","    seqs = tokenizer.texts_to_sequences(texts)\n","    maxlen = max(len(seq) for seq in seqs)\n","    return tf.keras.preprocessing.sequence.pad_sequences(seqs, padding='post', maxlen=maxlen)\n","\n","# 定义序列到序列模型\n","def seq2seq_model(input_vocab_size, output_vocab_size, hidden_units):\n","    # 定义编码器\n","    encoder_inputs = tf.keras.layers.Input(shape=(None,))\n","    encoder_embeddings = tf.keras.layers.Embedding(input_vocab_size, hidden_units)(encoder_inputs)\n","    encoder_lstm = tf.keras.layers.LSTM(hidden_units, return_state=True)\n","    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n","    encoder_states = [state_h, state_c]\n","\n","    # 定义解码器\n","    decoder_inputs = tf.keras.layers.Input(shape=(None,))\n","    decoder_embeddings = tf.keras.layers.Embedding(output_vocab_size, hidden_units)(decoder_inputs)\n","    decoder_lstm = tf.keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n","    decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n","    decoder_dense = tf.keras.layers.Dense(output_vocab_size, activation='softmax')\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","\n","    # 定义整个模型\n","    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","    return model\n","\n","# 模型评估和推理\n","def evaluate_model(model, eng_tokenizer, fra_tokenizer, sentence):\n","    \"\"\"\n","    Applies the model to translate a given sentence from English to French.\n","    \"\"\"\n","    sentence = preprocess_sentence(sentence)\n","    seqs = texts_to_sequences([sentence], eng_tokenizer)\n","    input_seq = seqs[0]\n","    states = model.encoder_model.predict(input_seq.reshape(1,-1))\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = fra_tokenizer.word_index['<start>']\n","    eos = fra_tokenizer.word_index['<end>']\n","    output_sentence = \"\"\n","    while True:\n","        output_tokens, h, c = model.decoder_model.predict([target_seq] + states)\n","        # 获取预测的下一个词\n","        idx = np.argmax(output_tokens[0, 0, :])\n","        # 结束条件\n","        if eos == idx or len(output_sentence) > 15:\n","            break\n","        word = \"\"\n","        # 根据序号转换成词\n","        if idx > 0:\n","            for w, i in fra_tokenizer.word_index.items():\n","                if i == idx:\n","                    word = w\n","                    break\n","        # 添加到输出语句中\n","        if len(word) > 0:\n","            if len(output_sentence) > 0:\n","                output_sentence += \" \"\n","            output_sentence += word\n","        # 更新解码器状态\n","        target_seq[0, 0] = idx\n","        states = [h, c]\n","    return output_sentence\n","\n","# 主程序\n","if __name__ == '__main__':\n","    # 预处理数据\n","    pairs = []\n","    for line in lines:\n","        parts = line.split('\\t')\n","        if len(parts) != 2:\n","            continue\n","        eng, fra = parts\n","        eng = preprocess_sentence(eng)\n","        fra = preprocess_sentence(fra)\n","        pairs.append((eng, fra))\n","\n","    # 划分训练和测试集\n","    eng_train, fra_train, eng_test, fra_test = split_train_test(pairs, test_size=0.1)\n","\n","    # 建立英语和法语的词汇表\n","    eng_tokenizer, eng_vocab_size = build_tokenizer(eng_train)\n","    fra_tokenizer, fra_vocab_size = build_tokenizer(fra_train)\n","\n","    # 将文本转换为数字\n","    eng_train_seqs = texts_to_sequences(eng_train, eng_tokenizer)\n","    fra_train_seqs = texts_to_sequences(fra_train, fra_tokenizer)\n","    eng_test_seqs = texts_to_sequences(eng_test, eng_tokenizer)\n","    fra_test_seqs = texts_to_sequences(fra_test, fra_tokenizer)\n","\n","    # 模型设置\n","    hidden_units = 256\n","    batch_size = 64\n","    num_epochs = 30\n","    val_split = 0.2\n","\n","    # 构建并训练模型\n","    model = seq2seq_model(eng_vocab_size, fra_vocab_size, hidden_units)\n","    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy')\n","    model.fit(x=[eng_train_seqs, fra_train_seqs[:, :-1]], y=fra_train_seqs[:, 1:], batch_size=batch_size, epochs=num_epochs, validation_split=val_split)\n","\n","    # 在测试集上评估模型\n","    loss = model.evaluate(x=[eng_test_seqs, fra_test_seqs[:, :-1]], y=fra_test_seqs[:, 1:])\n","    print(\"Test loss:\", loss)\n","\n","    # 进行翻译\n","    print(evaluate_model(model, eng_tokenizer, fra_tokenizer, \"I am a student.\"))\n","    print(evaluate_model(model, eng_tokenizer, fra_tokenizer, \"What is your name?\"))\n","    print(evaluate_model(model, eng_tokenizer, fra_tokenizer, \"How are you doing?\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"1WHDMu-v7iNN","executionInfo":{"status":"error","timestamp":1684919607656,"user_tz":-480,"elapsed":11980,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"776b0b38-3349-441e-fff3-7849fa6b85aa"},"execution_count":1,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-57ea4202fd7b>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# 划分训练和测试集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0meng_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfra_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfra_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# 建立英语和法语的词汇表\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-57ea4202fd7b>\u001b[0m in \u001b[0;36msplit_train_test\u001b[0;34m(pair_list, test_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[1;32m     29\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fra\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0meng_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."]}]},{"cell_type":"code","source":["# 读入数据集\n","data_path = \"./fra.txt\"\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split('\\n')\n","\n","import re\n","\n","def preprocess_sentence(sentence):\n","    \"\"\"\n","    Preprocesses a sentence by converting to lowercase, removing punctuation and non-alphabetic characters,\n","    and standardizing whitespace.\n","    \"\"\"\n","    sentence = sentence.lower().strip()\n","    sentence = re.sub(r\"[^a-zA-ZÀ-ÿ\\s']\", \"\", sentence)\n","    sentence = re.sub(r\"\\s+\", \" \", sentence)\n","    sentence = sentence.strip()\n","    return sentence\n","\n","def preprocess_pairs(pair_list):\n","    \"\"\"\n","    Preprocesses a list of sentence pairs.\n","    \"\"\"\n","    preprocessed_pairs = []\n","    for pair in pair_list:\n","        eng, fra = pair.split(\"\\t\")\n","        preprocessed_eng = preprocess_sentence(eng)\n","        preprocessed_fra = preprocess_sentence(fra)\n","        preprocessed_pairs.append((preprocessed_eng, preprocessed_fra))\n","    return preprocessed_pairs\n","\n","# Example usage\n","pair_list = [\"Go.\\tVa !\", \"Go.\\tMarche.\", \"Go.\\tEn route !\"]\n","preprocessed_pairs = preprocess_pairs(pair_list)\n","print(preprocessed_pairs)\n","\n","\n","# 划分训练和测试集\n","data = pd.DataFrame(preprocessed_pairs, columns=[\"eng\", \"fra\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEDcddDY9p21","executionInfo":{"status":"ok","timestamp":1684918845239,"user_tz":-480,"elapsed":377,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"f7bc6307-c71d-41ac-a8fc-4414c4bb937c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[('go', 'va'), ('go', 'marche'), ('go', 'en route')]\n"]}]},{"cell_type":"code","source":["data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"Cfa-2xDc9tk0","executionInfo":{"status":"ok","timestamp":1684918845939,"user_tz":-480,"elapsed":4,"user":{"displayName":"陈冰冰","userId":"11790981709466704139"}},"outputId":"02c1cebd-6d32-4ae0-a1e0-2e1c6af3c008"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  eng       fra\n","0  go        va\n","1  go    marche\n","2  go  en route"],"text/html":["\n","  <div id=\"df-537040ad-858c-4126-bea2-6908b55e3cf7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>eng</th>\n","      <th>fra</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>go</td>\n","      <td>va</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>go</td>\n","      <td>marche</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>go</td>\n","      <td>en route</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-537040ad-858c-4126-bea2-6908b55e3cf7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-537040ad-858c-4126-bea2-6908b55e3cf7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-537040ad-858c-4126-bea2-6908b55e3cf7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[],"metadata":{"id":"mUdrITYd-fQG"},"execution_count":null,"outputs":[]}]}