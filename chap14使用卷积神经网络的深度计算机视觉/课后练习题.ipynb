{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMq1aCA+YEVZhpdcufb3Bx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 课后练习题"],"metadata":{"id":"Cy0hpA4rKrX6"}},{"cell_type":"markdown","source":["## 1 与用于图像分类的全连接的DNN相比，CNN有什么优势。"],"metadata":{"id":"cfOA-BJlKxQt"}},{"cell_type":"markdown","source":["与用于图像分类的全连接的DNN相比，CNN有以下优势：\n","\n","局部连接：CNN中的每个神经元只与输入数据的一小部分(局部感受野)相连，这使得模型可以处理更大的输入数据量。\n","权重共享：在全连接的DNN网络中，每个神经元都有一组独立的权重参数，而在CNN中，卷积层中的权重参数在整个图像中都是共享的，这样可以减少参数数量，加快训练速度。\n","降采样：通过池化层，CNN可以减少神经元数量和计算成本，同时提高模型的抗噪能力和泛化能力。\n","适合处理高维数据：由于图像等数据的维度比较高，对于全连接的DNN来说，输入数据必须被拉平成一个长向量才能进行处理，而CNN能够保留数据的空间信息，从而更好地处理高维数据。\n","综上所述，CNN在图像处理任务中比全连接的DNN具有更好的性能和效率。"],"metadata":{"id":"onQesaSvKxT1"}},{"cell_type":"markdown","source":["## 2 考虑由3个卷积层组成的CNN，每个卷积层具有3*3内核，步幅为2和“same”填充，最底层输出100个特征图，中间层输出200个特征图，最顶层输出400个特征图。输入图像是200*200像素的rgb图像。CNN中的参数总数是多少？如果我们使用的是32位浮点数，那么在对单个实例进行预测时，至少该网络需要多少ram？训练一个包含50个图像的小批量时会怎样"],"metadata":{"id":"P1NhF84JKxWs"}},{"cell_type":"markdown","source":["假设所有卷积层的偏差项都包含在参数总数中，则：\n","\n","第一个卷积层包括：100个特征图，每个特征图包含 3x3个权重和1个偏差，共计 100x(3x3x3+1) = 2,800 个参数。\n","第二个卷积层包括：200个特征图，每个特征图由前一层的全部特征图卷积所得，因此每个特征图包括(3x3x100+1)个权重和1个偏差，共计 200x(3x3x100+1) = 1,802,000 个参数。\n","第三个卷积层包括：400个特征图，每个特征图由前一层的全部特征图卷积所得，因此每个特征图包括(3x3x200+1)个权重和一个偏差，共计 400x(3x3x200+1) = 2,721,600 个参数。\n","因此，整个CNN的参数总数为 2,800 + 1,802,000 + 2,721,600 = 4,526,400 个参数。\n","\n","在单个实例上预测时，至少需要在RAM中存储图像及输出特征图。每个特征图的大小为100x100（考虑填充和步幅），而每个特征图上每个像素点需要存储一个32位的浮点数。因此，每个特征图需要占用 100x100x4 = 40,000 个字节（或40 KB）的RAM。在前向传递过程中，CNN需要存储每个卷积层的中间结果。因此，CNN至少需要的RAM大小为 (100+200+400) x 40,000 = 24,000,000 字节（或24 MB）。\n","\n","用50个图像训练小批量，需要在RAM中存储这些图像和其对应的标签。假设图像和标签每个都是32位浮点数，同时考虑内存空间保留的余地，每个图像需要占用 200x200x3x4 = 480,000 字节（或0.48 MB），每个标签需要占用 4 个字节，则总共需要 50x(0.48+4) = 24.4 MB的RAM。当反向传播时，需要在RAM中存储CNN中每个参数的梯度。当CNN的某些层使用更高级的优化算法（如Adam）时，还需要在RAM中存储更多的中间结果。因此，训练过程中CNN需要使用的RAM大小比前向传递要大得多。"],"metadata":{"id":"Bq3ZzOisKxZT"}},{"cell_type":"markdown","source":["## 3 如果训练CNN时GPU内存不足，可以尝试哪五种方法来解决这个问题"],"metadata":{"id":"1c-duIjlKxcJ"}},{"cell_type":"markdown","source":["如果训练CNN时GPU内存不足，可以尝试以下五种方法来解决这个问题：\n","\n","降低批量大小：减小每个小批量中的图像数量，从而降低GPU内存使用量。但是，这可能会使学习过程变得更加不稳定。\n","减小图像大小：将输入图像的大小减小到可能的最小值，这样可以减少内存使用。\n","使用更小的模型：尝试使用更少的卷积层和/或更少的特征图，以及降低每个卷积层中过滤器的大小，这样可以减少模型的内存占用。\n","通过分布式训练使用多个GPU：使用多个GPU进行分布式训练，以增加可用内存。\n","使用混合精度训练技术：混合精度训练技术允许将半精度数字（float16）用于前向和后向传播，可以极大地减少内存使用，提高训练速度。"],"metadata":{"id":"4-VEIJLvKxef"}},{"cell_type":"markdown","source":["## 4 为什么要添加最大池化层而不是具有相同步幅的卷积层"],"metadata":{"id":"CFEXh4BqKxhG"}},{"cell_type":"markdown","source":["添加最大池化层而不是具有相同步幅的卷积层有以下几个原因：\n","\n","减少特征图的空间大小：对于具有较大步幅的卷积层，输出的特征图的空间大小会明显减小，这会减少后续的计算和数据量。但是多个卷积层的连续使用可能会导致特征图的空间大小降低过快，甚至直接将输入图像缩小到点。为了避免这种情况，可以使用最大池化层来改变下一层的空间大小，而不是使用具有相同步幅的卷积层。\n","提高模型的鲁棒性：最大池化层可以抑制小的偏差和图像噪声对模型的影响，从而增强模型的鲁棒性。通过选择适当的池化区域大小，最大池化层可以有效地实现这一点，而卷积层则很难达到这种效果。\n","减少过拟合：在训练深度卷积神经网络时，过拟合是一个常见的问题。添加最大池化层可以有效地减少这个问题，因为它可以强制模型学习更有代表性的特征并压缩参数。相比之下，使用具有相同步幅的卷积层可能会使模型更容易出现过拟合情况。"],"metadata":{"id":"ehOaYeQhKxj9"}},{"cell_type":"markdown","source":["## 5 你何时要添加局部响应归一化层？"],"metadata":{"id":"ITt9zIXyO-C_"}},{"cell_type":"markdown","source":["局部响应归一化（LRN）层的添加有时可以提高卷积神经网络的性能，具体情况如下：\n","\n","降低过拟合：在训练深度卷积神经网络时，过拟合是一个常见问题。添加LRN层可以使模型更具鲁棒性，防止模型过度拟合训练数据。\n","提高分类准确率：添加LRN层可以通过增加模型的非线性性来提高分类准确率，并促进神经元的稀疏表示。\n","在深度网络中实现更好的特征提取：随着深度网络的层数增加，每个神经元接收到的信息量也相应增加。添加LRN层可以使不同层次的特征之间具有更好的区分度，从而提高特征提取质量。\n","需要注意的是，LRN层对模型性能的影响通常取决于具体的任务和数据集。在某些情况下，LRN层可能会使性能下降，因此其使用需要谨慎评估和调试。"],"metadata":{"id":"UX3dGnrbO-LC"}},{"cell_type":"markdown","source":["## 6 与LeNET-5相比，你能说出AlexNet的主要创新之处吗？GoogleNet、ResNet、SENet和Xception的主要创新之处吗？"],"metadata":{"id":"-MAjMdt_O-RT"}},{"cell_type":"markdown","source":["AlexNet的主要创新之处：\n","\n","使用了更深的卷积网络，共包含8个卷积层和3个全连接层；\n","引入了Dropout技术以减少过拟合；\n","使用GPU加速训练。\n","GoogleNet的主要创新之处：\n","\n","使用了Inception模块，可以并行使用多个不同大小的卷积核，使得网络能够捕捉不同尺寸的特征；\n","提出了1x1卷积核的使用，可以在不增加计算负担的情况下增加非线性；\n","使用全局平均池化层代替全连接层，减少网络参数，避免过拟合。\n","ResNet的主要创新之处：\n","\n","提出了残差块（Residual Block），将前一层的特征图通过跨层连接（Skip Connection）的方式直接传递给后一层，由于可以保留之前层中的信息，可以通过更深的网络来获得更好的性能；\n","使用了批量归一化（Batch Normalization）技术，可以缩短收敛时间；\n","使用了更小的卷积核，进一步减少了网络参数数量。\n","SENet的主要创新之处：\n","\n","提出了Squeeze-and-Excitation（SE）模块，对通道特征进行加权，使得网络能够自适应地选取数据中最有用的特征；\n","可以与其他卷积网络结构结合使用，进一步提高网络性能。\n","Xception的主要创新之处：\n","\n","使用了深度可分离卷积（Depthwise Separable Convolution），将传统卷积分为两个步骤，先进行深度卷积，再进行空间卷积，可以减少参数数量；\n","构造了具有极少参数的网络，在模型复杂性与性能之间获得了平衡；\n","可以在计算资源有限的情况下进行快速推理。"],"metadata":{"id":"S0dZ9Dm1O-s5"}},{"cell_type":"markdown","source":["## 7 什么是全卷积网络？如何将密集层转换为卷积层？"],"metadata":{"id":"Ns76YxNXO-vf"}},{"cell_type":"markdown","source":["全卷积网络是一种卷积神经网络，可以输入任意大小的图像，并为每个像素生成一个预测结果。相对于传统的卷积神经网络，全卷积网络在最后一层使用卷积层而不是密集层，可以接受任意大小的输入。因此，全卷积网络对于图像分割任务非常有用。\n","\n","要将密集层转换为卷积层，可以将密集层理解为 ifeature map，然后通过充分连接层将其转换为 ofeature map。将这些权重重组为卷积核，可以将密集层转换为卷积层。该卷积层可以将多个 ifeature map 转换为多个 ofeature map，实现密集层的所有功能。\n","\n","具体而言，可以将密集层中的每个神经元视为一个 filter，多个 filter 就形成了一个 ifeature map，这样就可以将该层视为一个 1x1 大小的卷积层。然后使用这个转换后的层替换原有的密集层即可"],"metadata":{"id":"ICBcTaHNO-yG"}},{"cell_type":"markdown","source":["## 8 语义分割的主要技术困难是什么"],"metadata":{"id":"ETBlq13hO-0b"}},{"cell_type":"markdown","source":["语义分割是将图像中的每个像素标注为属于哪个语义类别的任务。与传统的图像分类不同，语义分割需要对每个像素进行分类，因此其主要技术困难包括以下几个方面：\n","\n","像素级别的标注：语义分割需要对每个像素进行分类，因此需要进行像素级别的标注，工作量较大且容易出现标注误差。\n","模型复杂度：与图像分类和目标检测相比，语义分割需要更复杂的模型来处理像素级别的分类，因此需要更多的计算资源和训练时间。\n","处理类别不平衡：由于不同语义类别在图像中出现的频率不同，因此可能存在类别不平衡问题，这会导致模型对少见类别的分类性能较差。\n","对非刚体和不规则形状的处理：语义分割的应用场景丰富多样，包括自然和非自然场景，在处理非刚体和不规则形状时，需要更高的模型灵活性。\n","针对这些技术困难，研究人员使用了许多技术和方法来改进语义分割模型，例如使用扩展数据集、利用残差结构、多尺度和融合上下文信息等。"],"metadata":{"id":"LjdYIoIXO_K2"}},{"cell_type":"markdown","source":["## 10 将迁移学习用于大图像分类，使用tensorflow数据集，将其分为训练集、验证集和测试集，构建输入流水线，包括适当的预处理操作，可选择地添加数据扩充。在此数据集微调预训练的模型"],"metadata":{"id":"OxrCVUWrO_NK"}},{"cell_type":"markdown","source":[],"metadata":{"id":"F8ufBRCSO_Pg"}}]}